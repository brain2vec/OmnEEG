{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91d84a47-219c-41db-b779-4a83d25cc430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/lustre06/project/6057506/dubadr/deephy/libs/OmnEEG\n"
     ]
    }
   ],
   "source": [
    "cd /home/dubadr/projects/def-gdumas85/dubadr/deephy/libs/OmnEEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67c72c1b-d5e0-4c2e-b12d-67b4e9565e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1422421/594928380.py:39: FutureWarning: mne.io.pick.pick_info is deprecated will be removed in 1.6, use documented public API instead. If no appropriate public API exists, please open an issue on GitHub.\n",
      "  from mne.io.pick import pick_info\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "EEG Processing Pipeline with Omneeg Integration\n",
    "\n",
    "This script can run omneeg on either raw data or preprocessed data.\n",
    "If preprocessing is requested, it will first run the preprocessing script\n",
    "and then apply omneeg transformation.\n",
    "\n",
    "Usage:\n",
    "    python eeg_pipeline.py database input_folder participant output_folder [options]\n",
    "\n",
    "Examples:\n",
    "    # Extract HBN data from tar.gz, then run omneeg on raw data\n",
    "    python eeg_pipeline.py HBN /data/hbn/tar_files participant_001 /output /path/to/demo.csv --mode raw --extract_data\n",
    "    \n",
    "    # Run full pipeline: extract -> preprocess -> omneeg\n",
    "    python eeg_pipeline.py HBN /data/hbn/tar_files participant_001 /output /path/to/demo.csv --mode preprocess --extract_data\n",
    "    \n",
    "    # Use already extracted/preprocessed data\n",
    "    python eeg_pipeline.py HBN /data/preprocessed participant_001 /output /path/to/demo.csv --mode preprocessed\n",
    "    \n",
    "    # Custom task name for extraction\n",
    "    python eeg_pipeline.py HBN /data/hbn/tar_files participant_001 /output /path/to/demo.csv --mode raw --extract_data --task MovieWatching\n",
    "    \n",
    "    # Use reconstruction mode (looks for labels.fif instead of RestingState_epo.fif)\n",
    "    python eeg_pipeline.py HBN /data/preprocessed participant_001 /output /path/to/demo.csv --mode preprocessed --reconstruction\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import subprocess\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "from mne.io.pick import pick_info\n",
    "\n",
    "chan_names_tobeused = ['E1', 'E2', 'E3', 'E4', 'E5', 'E6',\n",
    "                        'E7', 'E8', 'E9', 'E10', 'E11', 'E12', 'E13', 'E14', 'E15', 'E16',\n",
    "                        'E17', 'E18', 'E19', 'E20', 'E21', 'E22', 'E23', 'E24', 'E25',\n",
    "                        'E26', 'E27', 'E28', 'E29', 'E30', 'E31', 'E32', 'E33', 'E34',\n",
    "                        'E35', 'E36', 'E37', 'E38', 'E39', 'E40', 'E41', 'E42', 'E43',\n",
    "                        'E44', 'E45', 'E46', 'E47', 'E48', 'E49', 'E50', 'E51', 'E52',\n",
    "                        'E53', 'E54', 'E55', 'E56', 'E57', 'E58', 'E59', 'E60', 'E61',\n",
    "                        'E62', 'E63', 'E64', 'E65', 'E66', 'E67', 'E68', 'E69', 'E70',\n",
    "                        'E71', 'E72', 'E73', 'E74', 'E75', 'E76', 'E77', 'E78', 'E79',\n",
    "                        'E80', 'E81', 'E82', 'E83', 'E84', 'E85', 'E86', 'E87', 'E88',\n",
    "                        'E89', 'E90', 'E91', 'E92', 'E93', 'E94', 'E95', 'E96', 'E97',\n",
    "                        'E98', 'E99', 'E100', 'E101', 'E102', 'E103', 'E104', 'E105',\n",
    "                        'E106', 'E107', 'E108', 'E109', 'E110', 'E111', 'E112', 'E113',\n",
    "                        'E114', 'E115', 'E116', 'E117', 'E118', 'E119', 'E120', 'E121',\n",
    "                        'E122', 'E123', 'E124', 'E125', 'E126', 'E127', 'E128', 'Cz']\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('eeg_pipeline.log'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def extract_data(database, data_folder, participant_id, raw_folder, output_folder, task='RestingState'):\n",
    "    \"\"\"Extract HBN/CMI data from tar.gz files and create MNE Raw object\"\"\"\n",
    "    sampling_freq = 500\n",
    "    HBN_raw = dict()\n",
    "    \n",
    "    if database == 'HBN':\n",
    "        location_eeg = '/home/dubadr/projects/def-gdumas85/dubadr/deephy/datasets/HBN/GSN_HydroCel_129.sfp.txt'\n",
    "        path_to_events = f'{task}_event.csv'\n",
    "        path_to_data = f'{task}_data.csv'\n",
    "        # Define channel names for HBN - you may need to adjust this\n",
    "    else:\n",
    "        location_eeg = '/home/dubadr/projects/def-gdumas85/dubadr/deephy/datasets/CMI/Electrodes_position_cmi.txt'\n",
    "        path_to_events = f'{participant_id}001_events.csv'\n",
    "        path_to_data = f'{participant_id}001.csv'\n",
    "        # Define channel names for CMI - you may need to adjust this\n",
    "    \n",
    "    template_channel_names = np.loadtxt(location_eeg, dtype=str, usecols=0)\n",
    "    template_channel_locations = np.loadtxt(location_eeg, dtype=float, usecols=(1, 2, 3))\n",
    "    extract_folder = os.path.join(os.path.dirname(output_folder.rstrip('/')), 'scratch')\n",
    "    base_name = os.path.splitext(os.path.splitext(participant_id)[0])[0]\n",
    "    csvfolder = extract_folder + 'EEG/' + participant_id + '/EEG/raw/csv_format'\n",
    "    datafolder = data_folder + 'EEG/' + participant_id + '/EEG'\n",
    "\n",
    "    if not os.path.exists(os.path.join(extract_folder + 'EEG/' + participant_id)):\n",
    "        file = tarfile.open(data_folder + '/' + participant_id + '.tar.gz')\n",
    "        name = file.getnames()\n",
    "        file.extractall(extract_folder + 'EEG/')\n",
    "        file.close()\n",
    "\n",
    "    if os.path.exists(os.path.join(csvfolder, path_to_events)):\n",
    "        events = pd.read_csv(os.path.join(csvfolder, path_to_events))\n",
    "        if database == 'CMI':\n",
    "            events['type'] = events['type'].str.strip()\n",
    "            events = events[events['type'] != 'type']\n",
    "            events['type'] = events['type'].str.strip()\n",
    "            events['latency'] = pd.to_numeric(events['latency'])\n",
    "            events['urevent'] = pd.to_numeric(events['urevent'])\n",
    "\n",
    "        csvfile = os.path.join(csvfolder, path_to_data)\n",
    "        print('Starting data import')\n",
    "        data = pd.read_csv(csvfile, header=None)\n",
    "        data = data.values\n",
    "        print('Done data import')\n",
    "        data = data * 1e-6\n",
    "        chan_names = chan_names_tobeused\n",
    "\n",
    "        info = mne.create_info(chan_names, sfreq=sampling_freq, ch_types='eeg')\n",
    "        if database == 'HBN':\n",
    "            raw = mne.io.RawArray(data, info)\n",
    "            raw.drop_channels('Cz')\n",
    "        else:\n",
    "            channels_to_keep = [ch for ch in info['ch_names'] if ch != 'Cz']\n",
    "            new_info = pick_info(info, sel=[info['ch_names'].index(ch) for ch in channels_to_keep])\n",
    "            raw = mne.io.RawArray(data, new_info)\n",
    "\n",
    "        dt = np.dtype([('col0', 'U6'), ('col1', float), ('col2', float), ('col3', float)])\n",
    "        new_array = np.empty((132,), dtype=dt)\n",
    "        new_array['col0'] = template_channel_names\n",
    "        new_array['col1'] = template_channel_locations[:, 0]\n",
    "        new_array['col2'] = template_channel_locations[:, 1]\n",
    "        new_array['col3'] = template_channel_locations[:, 2]\n",
    "\n",
    "        txt_list = list(new_array['col0'])\n",
    "        raw_list = raw.info['ch_names']\n",
    "\n",
    "        uq = list(set(txt_list).symmetric_difference(set(raw_list)))\n",
    "\n",
    "        ID = []\n",
    "        for idx in range(min(3, len(uq))):  # Safety check\n",
    "            ID_temp = np.where(new_array['col0'] == uq[idx])\n",
    "            ID.append(ID_temp)\n",
    "\n",
    "        new_locs = np.delete(new_array, ID, 0)\n",
    "        template_locs_final = new_locs[['col1', 'col2', 'col3']]\n",
    "\n",
    "        # Handle MFF format if available\n",
    "        if os.path.exists(os.path.join(datafolder, 'raw', 'mff_format', participant_id)):\n",
    "            mff_folder = os.path.join(datafolder, 'raw', 'mff_format', participant_id)\n",
    "            os.rename(mff_folder, mff_folder + '.mff')\n",
    "            raw_mff = mne.io.read_raw_egi(mff_folder + '.mff')\n",
    "            os.rename(mff_folder + '.mff', mff_folder)\n",
    "\n",
    "            for i, ch in enumerate(raw.info['chs']):\n",
    "                name = ch['ch_name']\n",
    "                for ch_mff in raw_mff.info['chs']:\n",
    "                    if ch_mff['ch_name'] == name:\n",
    "                        break\n",
    "                ch['cal'] = ch_mff['cal']\n",
    "\n",
    "                if os.path.exists(os.path.join(csvfolder, f'{task}_chanlocs.csv')):\n",
    "                    # Note: channel_locations variable is not defined in the original function\n",
    "                    # You may need to load this separately\n",
    "                    pass  # ch['loc'][0:10] = channel_locations[:, i][1:11]\n",
    "                else:\n",
    "                    tlf = template_locs_final[i]\n",
    "                    ch['loc'][0:3] = np.array(list(tlf)).astype(object)\n",
    "\n",
    "                ch['kind'] = ch_mff['kind']\n",
    "                ch['unit'] = ch_mff['unit']\n",
    "                ch['coord_frame'] = ch_mff['coord_frame']\n",
    "\n",
    "        elif os.path.exists(os.path.join(csvfolder, f'{task}_chanlocs.csv')):\n",
    "            for i, ch in enumerate(raw.info['chs']):\n",
    "                ch['cal'] = 1.0e-06\n",
    "                # ch['loc'][0:10] = channel_locations[:, i][1:11]  # Need to define channel_locations\n",
    "                ch['kind'] = mne.utils._bunch.NamedInt('FIFFV_EEG_CH', 2)\n",
    "                ch['unit'] = mne.utils._bunch.NamedInt('FIFF_UNIT_V', 107)\n",
    "                ch['coord_frame'] = mne.utils._bunch.NamedInt('FIFFV_COORD_HEAD', 4)\n",
    "        else:\n",
    "            for i, ch in enumerate(raw.info['chs']):\n",
    "                ch['cal'] = 1.0e-06\n",
    "                tlf = template_locs_final[i]\n",
    "                ch['loc'][0:3] = np.array(list(tlf)).astype(object)\n",
    "                ch['kind'] = mne.utils._bunch.NamedInt('FIFFV_EEG_CH', 2)\n",
    "                ch['unit'] = mne.utils._bunch.NamedInt('FIFF_UNIT_V', 107)\n",
    "                ch['coord_frame'] = mne.utils._bunch.NamedInt('FIFFV_COORD_HEAD', 4)\n",
    "\n",
    "        if database == 'HBN':\n",
    "            descriptions = events.loc[:, 'type'].values\n",
    "            onsets = events.loc[:, 'sample'].values / raw.info['sfreq']\n",
    "            durations = events.loc[:, 'duration'].values\n",
    "            for index, description in enumerate(descriptions):\n",
    "                if description == 'break cnt':\n",
    "                    durations[index] = 0.1\n",
    "        else:\n",
    "            descriptions = events['type'].values\n",
    "            onsets = events['latency'].values / 500\n",
    "            durations = [1] * len(descriptions)\n",
    "\n",
    "        annotations = mne.Annotations(onsets, durations, descriptions)\n",
    "        raw.set_annotations(annotations)\n",
    "        \n",
    "        if not os.path.exists(raw_folder + '/' + participant_id):\n",
    "            os.makedirs(raw_folder + '/' + participant_id)\n",
    "        \n",
    "        # Save the raw data\n",
    "        raw.save(raw_folder + '/' + participant_id + '/' + task + '.fif', overwrite=True)\n",
    "        \n",
    "        HBN_raw[participant_id + '_' + task] = raw\n",
    "    else:\n",
    "        print(\"there is no data for this participant \" + participant_id)\n",
    "        return None\n",
    "\n",
    "    return raw\n",
    "\n",
    "class EEGPipeline:\n",
    "    \"\"\"Main pipeline class for EEG processing with omneeg\"\"\"\n",
    "    \n",
    "    def __init__(self, database, input_folder, participant, output_folder, \n",
    "                 preprocessing_script='PPSPrep/preprocessing.py', omneeg_script='omneeg.py', \n",
    "                 demo_file=None, transform_module='omneeg.transform', extract_data_first=False):\n",
    "        self.database = database\n",
    "        self.input_folder = Path(input_folder)\n",
    "        self.participant = participant\n",
    "        self.output_folder = Path(output_folder)\n",
    "        self.preprocessing_script = Path(preprocessing_script)\n",
    "        self.omneeg_script = Path(omneeg_script)\n",
    "        self.demo_file = demo_file\n",
    "        self.transform_module = transform_module\n",
    "        self.extract_data_first = extract_data_first\n",
    "        \n",
    "        # Create output folder structure\n",
    "        self.setup_output_folders()\n",
    "    \n",
    "    def run_data_extraction(self, task='RestingState'):\n",
    "        \"\"\"Extract data from tar.gz files for HBN/CMI databases\"\"\"\n",
    "        if self.database not in ['HBN', 'CMI']:\n",
    "            logger.info(f\"Data extraction not needed for database: {self.database}\")\n",
    "            return True\n",
    "            \n",
    "        logger.info(f\"Starting data extraction for {self.database} participant {self.participant}\")\n",
    "        \n",
    "        try:\n",
    "            raw_folder = self.output_folder / 'raw'\n",
    "            raw = extract_data(\n",
    "                database=self.database,\n",
    "                data_folder=str(self.input_folder),\n",
    "                participant_id=self.participant,\n",
    "                raw_folder=str(raw_folder),\n",
    "                output_folder=str(self.output_folder),\n",
    "                task=task\n",
    "            )\n",
    "            \n",
    "            if raw is not None:\n",
    "                logger.info(\"Data extraction completed successfully\")\n",
    "                return True\n",
    "            else:\n",
    "                logger.error(\"Data extraction failed - no data returned\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Data extraction failed: {str(e)}\")\n",
    "            import traceback\n",
    "            logger.error(traceback.format_exc())\n",
    "            return False\n",
    "    \n",
    "    def setup_output_folders(self):\n",
    "        \"\"\"Create necessary output folder structure\"\"\"\n",
    "        folders = ['raw', 'ica', 'process', 'log', 'omneeg']\n",
    "        for folder in folders:\n",
    "            folder_path = self.output_folder / folder\n",
    "            folder_path.mkdir(parents=True, exist_ok=True)\n",
    "            logger.info(f\"Created/verified folder: {folder_path}\")\n",
    "    \n",
    "    def run_preprocessing(self, remove_channels=False):\n",
    "        \"\"\"Run the preprocessing script\"\"\"\n",
    "        logger.info(f\"Starting preprocessing for participant {self.participant}\")\n",
    "        \n",
    "        # Check if preprocessing script exists\n",
    "        if not self.preprocessing_script.exists():\n",
    "            logger.error(f\"Preprocessing script not found: {self.preprocessing_script}\")\n",
    "            logger.info(\"Please ensure the PPSPrep directory and preprocessing.py file exist\")\n",
    "            return False\n",
    "        \n",
    "        # Construct preprocessing command\n",
    "        cmd = [\n",
    "            'python', str(self.preprocessing_script),\n",
    "            self.database,\n",
    "            str(self.input_folder),\n",
    "            self.participant,\n",
    "            str(self.output_folder),\n",
    "            'output_placeholder',  # This seems to be unused in the original script\n",
    "            '--remove_channels', str(remove_channels).lower()\n",
    "        ]\n",
    "        \n",
    "        logger.info(f\"Running preprocessing command: {' '.join(cmd)}\")\n",
    "        \n",
    "        try:\n",
    "            result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
    "            logger.info(\"Preprocessing completed successfully\")\n",
    "            logger.info(f\"Preprocessing stdout: {result.stdout}\")\n",
    "            if result.stderr:\n",
    "                logger.warning(f\"Preprocessing stderr: {result.stderr}\")\n",
    "            return True\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            logger.error(f\"Preprocessing failed with return code {e.returncode}\")\n",
    "            logger.error(f\"Error output: {e.stderr}\")\n",
    "            logger.error(f\"Standard output: {e.stdout}\")\n",
    "            return False\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"Python interpreter or preprocessing script not found\")\n",
    "            logger.error(f\"Script path: {self.preprocessing_script}\")\n",
    "            return False\n",
    "    \n",
    "    def find_input_file(self, mode):\n",
    "        \"\"\"Find the appropriate input file based on processing mode\"\"\"\n",
    "        if mode == 'raw':\n",
    "            # For HBN with extraction, look for extracted .fif file first\n",
    "            if self.database in ['HBN', 'CMI'] and self.extract_data_first:\n",
    "                extracted_file = self.output_folder / 'raw' / self.participant / 'RestingState.fif'\n",
    "                if extracted_file.exists():\n",
    "                    return extracted_file\n",
    "            \n",
    "            # Look for raw data files in input folder\n",
    "            possible_extensions = ['.fif', '.set', '.edf', '.bdf', '.mat', '.tar.gz']\n",
    "            for ext in possible_extensions:\n",
    "                pattern = f\"*{self.participant}*{ext}\"\n",
    "                files = list(self.input_folder.glob(pattern))\n",
    "                if files:\n",
    "                    return files[0]\n",
    "                \n",
    "                # For HBN, also look for tar.gz files with just participant name\n",
    "                if ext == '.tar.gz':\n",
    "                    tar_file = self.input_folder / f\"{self.participant}.tar.gz\"\n",
    "                    if tar_file.exists():\n",
    "                        return tar_file\n",
    "            \n",
    "            # If no specific participant file found, look for generic patterns\n",
    "            for ext in possible_extensions[:-1]:  # Exclude .tar.gz for generic search\n",
    "                files = list(self.input_folder.glob(f\"*{ext}\"))\n",
    "                if files:\n",
    "                    logger.warning(f\"No specific participant file found, using: {files[0]}\")\n",
    "                    return files[0]\n",
    "        \n",
    "        elif mode == 'preprocessed':\n",
    "            # Look for preprocessed data in output folder\n",
    "            ica_file = self.output_folder / 'ica' / self.participant / 'ica.fif'\n",
    "            process_file = self.output_folder / 'process' / self.participant / 'RestingState_epo.fif'\n",
    "            \n",
    "            if process_file.exists():\n",
    "                return process_file\n",
    "            elif ica_file.exists():\n",
    "                return ica_file\n",
    "        \n",
    "        elif mode == 'preprocess':\n",
    "            # After preprocessing, use the processed file\n",
    "            process_file = self.output_folder / 'process' / self.participant / 'RestingState_epo.fif'\n",
    "            if process_file.exists():\n",
    "                return process_file\n",
    "            \n",
    "            # Fallback to ICA file\n",
    "            ica_file = self.output_folder / 'ica' / self.participant / 'ica.fif'\n",
    "            if ica_file.exists():\n",
    "                return ica_file\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def run_omneeg(self, input_file, output_suffix='transformed', reconstruction=False, use_transform_module=True):\n",
    "        \"\"\"Run omneeg transformation on the input file\"\"\"\n",
    "        logger.info(f\"Starting omneeg transformation on {input_file}\")\n",
    "        \n",
    "        if use_transform_module:\n",
    "            # Use the Interpolate class from omneeg/transform.py directly\n",
    "            return self._run_omneeg_direct(input_file, output_suffix, reconstruction)\n",
    "        else:\n",
    "            # Use the original omneeg.py script\n",
    "            return self._run_omneeg_script(input_file, output_suffix, reconstruction)\n",
    "    \n",
    "    def _run_omneeg_direct(self, input_file, output_suffix='transformed', reconstruction=False):\n",
    "        \"\"\"Run omneeg transformation using direct import of Interpolate class\"\"\"\n",
    "        try:\n",
    "            # Import required modules\n",
    "            import mne\n",
    "            import numpy as np\n",
    "            import h5py\n",
    "            import pandas as pd\n",
    "            from importlib import import_module\n",
    "            \n",
    "            # Import the Interpolate class from omneeg/transform.py\n",
    "            try:\n",
    "                transform_module = import_module(self.transform_module)\n",
    "                Interpolate = transform_module.Interpolate\n",
    "                logger.info(f\"Successfully imported Interpolate from {self.transform_module}\")\n",
    "            except ImportError as e:\n",
    "                logger.error(f\"Failed to import Interpolate from {self.transform_module}: {e}\")\n",
    "                return None\n",
    "            \n",
    "            # Try to import demo function, but handle gracefully if it fails\n",
    "            try:\n",
    "                from demo import get_demo\n",
    "                use_demo = True\n",
    "                logger.info(\"Successfully imported get_demo from demo module\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not import get_demo from demo module: {e}\")\n",
    "                logger.info(\"Will create minimal demographics instead\")\n",
    "                use_demo = False\n",
    "            \n",
    "            # Construct output file path\n",
    "            omneeg_output_folder = self.output_folder / 'omneeg'\n",
    "            omneeg_output_folder.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # For epochs created from raw data, use the input_file directly\n",
    "            participant_file = input_file\n",
    "            \n",
    "            logger.info(f\"Loading epochs from: {participant_file}\")\n",
    "            \n",
    "            # Load epochs\n",
    "            # Load epochs - check if it's a raw file or epochs file\n",
    "            import warnings\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings(\"ignore\", message=\"This filename (.*) does not conform to MNE naming conventions\")\n",
    "\n",
    "                # Check if we're loading epochs or raw data\n",
    "                if 'epochs' in str(participant_file) or str(participant_file).endswith('_epo.fif'):\n",
    "                    # This is an epochs file\n",
    "                    epochs = mne.read_epochs(str(participant_file), preload=True)\n",
    "                else:\n",
    "                    # This is a raw file, create epochs from it\n",
    "                    logger.info(\"Input appears to be raw data, creating fixed-length epochs\")\n",
    "                    raw = mne.io.read_raw_fif(str(participant_file), preload=True)\n",
    "                    epochs = mne.make_fixed_length_epochs(raw, duration=2.0, preload=True)\n",
    "            \n",
    "            # Resample to 500 Hz\n",
    "            epochs.resample(500)\n",
    "            \n",
    "            # Get demographics\n",
    "            if use_demo and self.demo_file:\n",
    "                try:\n",
    "                    df_demo = get_demo(self.database, str(self.input_folder), self.participant, self.demo_file)\n",
    "                    logger.info(\"Successfully retrieved demographics from demo module\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to get demographics from demo module: {e}\")\n",
    "                    # Create minimal demographics\n",
    "                    df_demo = pd.DataFrame({\n",
    "                        'participant_id': [self.participant],\n",
    "                        'database': [self.database]\n",
    "                    })\n",
    "            else:\n",
    "                # Create minimal demographics\n",
    "                logger.info(\"Creating minimal demographics\")\n",
    "                df_demo = pd.DataFrame({\n",
    "                    'participant_id': [self.participant],\n",
    "                    'database': [self.database]\n",
    "                })\n",
    "            \n",
    "            logger.info(f\"Channel names: {epochs.info['ch_names'][:5]}...\")  # Show first 5 channels\n",
    "            \n",
    "            # Apply interpolation transformation\n",
    "            logger.info(\"Applying Interpolate transformation (32x32)\")\n",
    "            interpolate = Interpolate((32, 32))\n",
    "            data_temp = interpolate(epochs)\n",
    "            \n",
    "            # Construct output path\n",
    "            participant_id = self._get_participant_id()\n",
    "            h5_path = omneeg_output_folder / f\"{participant_id}.h5\"\n",
    "            \n",
    "            # Save to HDF5\n",
    "            logger.info(f\"Saving transformed data to: {h5_path}\")\n",
    "            with h5py.File(h5_path, 'w') as f:\n",
    "                # Save EEG array\n",
    "                f.create_dataset('power_data', data=data_temp, compression=\"gzip\")\n",
    "                \n",
    "                # Save demographics as a group of attributes (one key per column)\n",
    "                demo_group = f.create_group('demographics')\n",
    "                for key, value in df_demo.iloc[0].items():  # assumes 1-row per participant\n",
    "                    demo_group.attrs[key] = value\n",
    "            \n",
    "            logger.info(\"Omneeg transformation completed successfully using direct method\")\n",
    "            return h5_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Direct omneeg transformation failed: {str(e)}\")\n",
    "            import traceback\n",
    "            logger.error(traceback.format_exc())\n",
    "            return None\n",
    "    \n",
    "    def _find_participant_file(self, data_type, reconstruction=False):\n",
    "        \"\"\"Find the participant file based on database type and processing mode\"\"\"\n",
    "        if self.database == 'HBN':\n",
    "            return self.output_folder / 'process' / self.participant / data_type\n",
    "        \n",
    "        elif self.database == 'BCAN':\n",
    "            # For BCAN, we need to handle the demo file logic\n",
    "            if self.demo_file:\n",
    "                import pandas as pd\n",
    "                demo = pd.read_csv(self.demo_file)\n",
    "                basename = os.path.basename(os.path.normpath(self.participant))\n",
    "                participant_id = basename.split('_', 1)[1] if '_' in basename else basename\n",
    "                index_participant = demo[demo['Participant code'] == participant_id].index\n",
    "                if len(index_participant) > 0:\n",
    "                    Record_id = demo.iloc[index_participant[0]]['Record ID']\n",
    "                    Participant_code = demo.iloc[index_participant[0]]['Participant code']\n",
    "                    folder_name = f\"{Record_id}_{Participant_code}\"\n",
    "                    return self.output_folder / 'process' / folder_name / data_type\n",
    "            return self.output_folder / 'process' / self.participant / data_type\n",
    "        \n",
    "        elif self.database in ['VIP', 'RDB', 'ABCCT', 'XFRAGILE', 'NED', 'HSJ', 'TUEG']:\n",
    "            participant_id = self.participant.split('.')[0] if self.database == 'VIP' else self.participant\n",
    "            if self.database == 'RDB':\n",
    "                participant_id = self.participant[:-7]\n",
    "            else:\n",
    "                participant_id = os.path.basename(self.participant)\n",
    "            return self.output_folder / 'process' / participant_id / data_type\n",
    "        \n",
    "        else:\n",
    "            # Default case\n",
    "            return self.output_folder / 'process' / self.participant / data_type\n",
    "    \n",
    "    def _get_participant_id(self):\n",
    "        \"\"\"Get the correct participant ID based on database type\"\"\"\n",
    "        if self.database == 'HBN':\n",
    "            return self.participant\n",
    "        elif self.database == 'BCAN':\n",
    "            basename = os.path.basename(os.path.normpath(self.participant))\n",
    "            return basename\n",
    "        elif self.database == 'VIP':\n",
    "            return self.participant.split('.')[0]\n",
    "        elif self.database == 'RDB':\n",
    "            return self.participant[:-7]\n",
    "        else:\n",
    "            return os.path.basename(self.participant)\n",
    "    \n",
    "    def _run_omneeg_script(self, input_file, output_suffix='transformed', reconstruction=False):\n",
    "        \"\"\"Run omneeg transformation using the original omneeg.py script\"\"\"\n",
    "        # Check if omneeg script exists\n",
    "        if not self.omneeg_script.exists():\n",
    "            logger.error(f\"Omneeg script not found: {self.omneeg_script}\")\n",
    "            return None\n",
    "        \n",
    "        # Construct output file path - omneeg saves as .h5 files\n",
    "        omneeg_output_folder = self.output_folder / 'omneeg'\n",
    "        omneeg_output_folder.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # For omneeg, we need to determine the correct input folder based on the mode\n",
    "        if 'process' in str(input_file):\n",
    "            # Using preprocessed data from the process folder\n",
    "            data_folder = self.output_folder / 'process'\n",
    "        elif 'ica' in str(input_file):\n",
    "            # Using ICA processed data\n",
    "            data_folder = self.output_folder / 'ica'\n",
    "        else:\n",
    "            # Using raw data\n",
    "            data_folder = self.input_folder\n",
    "        \n",
    "        # Construct omneeg command based on the actual omneeg.py script\n",
    "        cmd = [\n",
    "            'python', str(self.omneeg_script),\n",
    "            self.database,                    # database\n",
    "            str(data_folder),                 # input_folder (data_folder in omneeg)\n",
    "            self.participant,                 # input_path (participant in omneeg)\n",
    "            str(omneeg_output_folder),        # output_folder\n",
    "            'output_placeholder',             # output_file (seems unused in omneeg)\n",
    "            str(self.demo_file) if self.demo_file else 'demo_placeholder',  # demo_file\n",
    "            '--reconstruction', str(reconstruction).lower()  # reconstruction flag\n",
    "        ]\n",
    "        \n",
    "        logger.info(f\"Running omneeg command: {' '.join(cmd)}\")\n",
    "        \n",
    "        try:\n",
    "            result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
    "            \n",
    "            # Omneeg saves as participant_id.h5 in the output folder\n",
    "            output_file = omneeg_output_folder / f\"{self.participant}.h5\"\n",
    "            \n",
    "            logger.info(\"Omneeg transformation completed successfully\")\n",
    "            logger.info(f\"Output saved to: {output_file}\")\n",
    "            logger.info(f\"Omneeg stdout: {result.stdout}\")\n",
    "            if result.stderr:\n",
    "                logger.warning(f\"Omneeg stderr: {result.stderr}\")\n",
    "            \n",
    "            return output_file\n",
    "            \n",
    "        except subprocess.CalledProcessError as e:\n",
    "            logger.error(f\"Omneeg transformation failed with return code {e.returncode}\")\n",
    "            logger.error(f\"Error output: {e.stderr}\")\n",
    "            logger.error(f\"Standard output: {e.stdout}\")\n",
    "            return None\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"Python interpreter or omneeg script not found\")\n",
    "            logger.error(f\"Script path: {self.omneeg_script}\")\n",
    "            return None\n",
    "    \n",
    "    def run_pipeline(self, mode='preprocess', remove_channels=False, reconstruction=False, use_transform_module=True, task='RestingState'):\n",
    "        \"\"\"Run the complete pipeline based on specified mode\"\"\"\n",
    "        logger.info(f\"Starting EEG pipeline in '{mode}' mode for participant {self.participant}\")\n",
    "        \n",
    "        # Step 1: Data extraction (if needed and requested)\n",
    "        if self.extract_data_first and self.database in ['HBN', 'CMI']:\n",
    "            if not self.run_data_extraction(task=task):\n",
    "                logger.error(\"Data extraction failed, aborting pipeline\")\n",
    "                return False\n",
    "        \n",
    "        if mode == 'preprocess':\n",
    "            # Run preprocessing first\n",
    "            if not self.run_preprocessing(remove_channels=remove_channels):\n",
    "                logger.error(\"Preprocessing failed, aborting pipeline\")\n",
    "                return False\n",
    "            \n",
    "            # Find processed file\n",
    "            input_file = self.find_input_file('preprocess')\n",
    "            if not input_file:\n",
    "                logger.error(\"Could not find preprocessed file after preprocessing\")\n",
    "                return False\n",
    "            \n",
    "            # Run omneeg on preprocessed data\n",
    "            output_file = self.run_omneeg(input_file, 'preprocessed_transformed', \n",
    "                                        reconstruction=reconstruction, use_transform_module=use_transform_module)\n",
    "            \n",
    "        elif mode == 'raw':\n",
    "            # Find raw input file\n",
    "            input_file = self.find_input_file('raw')\n",
    "            if not input_file:\n",
    "                logger.error(\"Could not find raw input file\")\n",
    "                return False\n",
    "            \n",
    "            # Run omneeg on raw data\n",
    "            output_file = self.run_omneeg(input_file, 'raw_transformed', \n",
    "                                        reconstruction=reconstruction, use_transform_module=use_transform_module)\n",
    "            \n",
    "        elif mode == 'preprocessed':\n",
    "            # Find already preprocessed file\n",
    "            input_file = self.find_input_file('preprocessed')\n",
    "            if not input_file:\n",
    "                logger.error(\"Could not find preprocessed file\")\n",
    "                return False\n",
    "            \n",
    "            # Run omneeg on preprocessed data\n",
    "            output_file = self.run_omneeg(input_file, 'preprocessed_transformed', \n",
    "                                        reconstruction=reconstruction, use_transform_module=use_transform_module)\n",
    "        \n",
    "        else:\n",
    "            logger.error(f\"Unknown mode: {mode}\")\n",
    "            return False\n",
    "        \n",
    "        if output_file:\n",
    "            logger.info(f\"Pipeline completed successfully. Output: {output_file}\")\n",
    "            return True\n",
    "        else:\n",
    "            logger.error(\"Pipeline failed\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b37ba803-96fa-467d-b196-fb410fc3f510",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--mode {raw,preprocess,preprocessed}]\n",
      "                             [--remove_channels] [--reconstruction]\n",
      "                             [--preprocessing_script PREPROCESSING_SCRIPT]\n",
      "                             [--omneeg_script OMNEEG_SCRIPT]\n",
      "                             [--transform_module TRANSFORM_MODULE]\n",
      "                             [--use_script] [--extract_data] [--task TASK]\n",
      "                             [--log_level {DEBUG,INFO,WARNING,ERROR}]\n",
      "                             database input_folder participant output_folder\n",
      "                             demo_file\n",
      "ipykernel_launcher.py: error: the following arguments are required: input_folder, participant, output_folder, demo_file\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dubadr/pyvista_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8710acf6-17ee-452f-b86b-6014e99c0e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-18 11:30:07,621 - __main__ - INFO - Created/verified folder: /home/dubadr/scratch/2seconds/HBN/raw\n",
      "2025-08-18 11:30:07,624 - __main__ - INFO - Created/verified folder: /home/dubadr/scratch/2seconds/HBN/ica\n",
      "2025-08-18 11:30:07,626 - __main__ - INFO - Created/verified folder: /home/dubadr/scratch/2seconds/HBN/process\n",
      "2025-08-18 11:30:07,628 - __main__ - INFO - Created/verified folder: /home/dubadr/scratch/2seconds/HBN/log\n",
      "2025-08-18 11:30:07,629 - __main__ - INFO - Created/verified folder: /home/dubadr/scratch/2seconds/HBN/omneeg\n"
     ]
    }
   ],
   "source": [
    "pipeline = EEGPipeline(\n",
    "    database='HBN',\n",
    "    input_folder='/home/dubadr/projects/ctb-gdumas85/data/HBN/EEG',  # Note: should be the parent folder\n",
    "    participant='NDARAA075AMK',  # Just the participant ID\n",
    "    output_folder='/home/dubadr/scratch/2seconds/HBN',\n",
    "    demo_file='/home/dubadr/scratch/2seconds/HBN/data.csv',\n",
    "    extract_data_first=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75ecf377-a6fa-4d95-8a4c-33c8bb1e291f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.EEGPipeline at 0x145f35967310>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "073016d1-0246-43f9-93d0-5f4a96eb7b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-18 11:30:09,283 - __main__ - INFO - Starting EEG pipeline in 'raw' mode for participant NDARAA075AMK\n",
      "2025-08-18 11:30:09,284 - __main__ - INFO - Starting data extraction for HBN participant NDARAA075AMK\n",
      "Starting data import\n",
      "Done data import\n",
      "Creating RawArray with float64 data, n_channels=129, n_times=182086\n",
      "    Range : 0 ... 182085 =      0.000 ...   364.170 secs\n",
      "Ready.\n",
      "Overwriting existing file.\n",
      "Writing /home/dubadr/scratch/2seconds/HBN/raw/NDARAA075AMK/RestingState.fif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1422421/594928380.py:205: RuntimeWarning: This filename (/home/dubadr/scratch/2seconds/HBN/raw/NDARAA075AMK/RestingState.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  raw.save(raw_folder + '/' + participant_id + '/' + task + '.fif', overwrite=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing /home/dubadr/scratch/2seconds/HBN/raw/NDARAA075AMK/RestingState.fif\n",
      "[done]\n",
      "2025-08-18 11:31:06,979 - __main__ - INFO - Data extraction completed successfully\n",
      "2025-08-18 11:31:06,982 - __main__ - INFO - Starting omneeg transformation on /home/dubadr/scratch/2seconds/HBN/raw/NDARAA075AMK/RestingState.fif\n",
      "2025-08-18 11:31:07,076 - __main__ - INFO - Successfully imported Interpolate from omneeg.transform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre06/project/6057506/dubadr/deephy/libs/OmnEEG/omneeg/transform.py:4: FutureWarning: mne.io.pick.pick_types is deprecated will be removed in 1.6, use documented public API instead. If no appropriate public API exists, please open an issue on GitHub.\n",
      "  from mne.io.pick import pick_types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-18 11:31:09,160 - __main__ - WARNING - Could not import get_demo from demo module: list index out of range\n",
      "2025-08-18 11:31:09,162 - __main__ - INFO - Will create minimal demographics instead\n",
      "2025-08-18 11:31:09,164 - __main__ - INFO - Loading epochs from: /home/dubadr/scratch/2seconds/HBN/raw/NDARAA075AMK/RestingState.fif\n",
      "2025-08-18 11:31:09,165 - __main__ - INFO - Input appears to be raw data, creating fixed-length epochs\n",
      "Opening raw data file /home/dubadr/scratch/2seconds/HBN/raw/NDARAA075AMK/RestingState.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 182085 =      0.000 ...   364.170 secs\n",
      "Ready.\n",
      "Reading 0 ... 182085  =      0.000 ...   364.170 secs...\n",
      "Not setting metadata\n",
      "182 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 182 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Sampling frequency of the instance is already 500.0, returning unmodified.\n",
      "2025-08-18 11:31:09,415 - __main__ - INFO - Creating minimal demographics\n",
      "2025-08-18 11:31:09,417 - __main__ - INFO - Channel names: ['E1', 'E2', 'E3', 'E4', 'E5']...\n",
      "2025-08-18 11:31:09,418 - __main__ - INFO - Applying Interpolate transformation (32x32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre06/project/6057506/dubadr/deephy/libs/OmnEEG/omneeg/transform.py:42: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  data = eeg.get_data()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-18 11:32:21,396 - __main__ - INFO - Saving transformed data to: /home/dubadr/scratch/2seconds/HBN/omneeg/NDARAA075AMK.h5\n",
      "2025-08-18 11:40:01,748 - __main__ - INFO - Omneeg transformation completed successfully using direct method\n",
      "2025-08-18 11:40:01,756 - __main__ - INFO - Pipeline completed successfully. Output: /home/dubadr/scratch/2seconds/HBN/omneeg/NDARAA075AMK.h5\n"
     ]
    }
   ],
   "source": [
    "success = pipeline.run_pipeline(\n",
    "    mode='raw',\n",
    "    remove_channels=False,\n",
    "    reconstruction=False,\n",
    "    use_transform_module=True  # Use direct import method\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0037b92b-71bd-4a07-80a4-78c60c7d038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "python eeg_pipeline.py HBN /data/raw participant_001 /output /path/to/demo.csv --mode raw"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyvista_env)",
   "language": "python",
   "name": "pyvista_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
